{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a735ee1",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03418de",
   "metadata": {},
   "source": [
    "In this notebook we evaluate the accuracy of the predicted alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67636de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from plotnine import *\n",
    "from pandas.api.types import CategoricalDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET = 'toy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5564e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_ROOT = Path('../ttmp/Chopin_Mazurkas_Modified/annotations_beat')\n",
    "query_list = Path(f'cfg_files/queries.train.{TRAIN_SET}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70369f24",
   "metadata": {},
   "source": [
    "### Evaluate hypothesis directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d9416",
   "metadata": {},
   "source": [
    "First evaluate a single hypothesis directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ed995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dir(hypdir, querylist, annot_root_1, annot_root_2, hop_sec, savefile = None):\n",
    "    allErrs = {}\n",
    "    cnt = 0\n",
    "    print(f'Processing {hypdir} ', end='')\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            assert len(parts) == 2\n",
    "            basename = os.path.basename(parts[0]) + '__' + os.path.basename(parts[1])\n",
    "            hypfile = hypdir + '/' + basename + '.pkl'\n",
    "            if not os.path.exists(hypfile):\n",
    "                print(\"X\", end='')\n",
    "                continue\n",
    "            allErrs[basename] = eval_file(hypfile, annot_root_1, annot_root_2, hop_sec)\n",
    "            cnt += 1\n",
    "            if cnt % 500 == 0:\n",
    "                print(\".\", end='')\n",
    "    print(' done')\n",
    "    if savefile:\n",
    "        pickle.dump(allErrs, open(savefile, 'wb'))\n",
    "        \n",
    "    return allErrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9147908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_file(hypfile, annot_root_1, annot_root_2, hop_sec):\n",
    "    parts = os.path.basename(hypfile).split('__')\n",
    "    assert len(parts) == 2\n",
    "    piece = extractPieceName(parts[0])\n",
    "    annotfile1 = (annot_root_1 / piece / parts[0]).with_suffix('.beat')\n",
    "    annotfile2 = (annot_root_2 / piece / parts[1]).with_suffix('.beat')\n",
    "    \n",
    "    # Irmak\n",
    "    gt1, gt2 = getTimestamps(annotfile1, annotfile2)\n",
    "    hypalign = loadAlignment(hypfile) # warping path in frames\n",
    "\n",
    "    if hypalign is None:\n",
    "        err = np.array(np.ones(gt1.shape) * np.inf)\n",
    "#         err = [] # no valid path\n",
    "    else:\n",
    "        pred2 = np.interp(gt1, hypalign[0,:]*hop_sec, hypalign[1,:]*hop_sec)\n",
    "        err = pred2 - gt2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPieceName(fullpath):\n",
    "    basename = os.path.basename(fullpath) # e.g. Chopin_Op068No3_Sztompka-1959_pid9170b-21\n",
    "    parts = basename.split('_')\n",
    "    piece = '_'.join(parts[0:2]) # e.g. Chopin_Op068No3\n",
    "    return piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimestamps(annotfile1, annotfile2):\n",
    "    df1 = pd.read_csv(annotfile1, header=None, sep='\\s+', skiprows=3) \n",
    "    df2 = pd.read_csv(annotfile2, header=None, sep='\\s+', skiprows=3)\n",
    "    \n",
    "    df_merged = pd.merge(df1, df2, on=[2], how='inner')\n",
    "\n",
    "    return df_merged['0_x'], df_merged['0_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db299879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAlignment(hypfile):\n",
    "    with open(hypfile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c775cb3",
   "metadata": {},
   "source": [
    "Evaluate all hypothesis directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_dirs(rootdir, querylist, hop_sec, outdir, system, benchmark):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    if 'partial' in benchmark:\n",
    "        annot_root_1 = Path(f'../ttmp/Chopin_Mazurkas_Benchmarks/partialStart/annotations_beat')\n",
    "        annot_root_2 = Path(f'../ttmp/Chopin_Mazurkas_Benchmarks/partialEnd/annotations_beat')\n",
    "    elif benchmark == 'matching':\n",
    "        annot_root_1 = Path(f'../ttmp/Chopin_Mazurkas_Modified/annotations_beat')\n",
    "        annot_root_2 = Path(f'../ttmp/Chopin_Mazurkas_Modified/annotations_beat')\n",
    "    else:\n",
    "        annot_root_1 = Path(f'../ttmp/Chopin_Mazurkas_Benchmarks/{benchmark}/annotations_beat')\n",
    "        annot_root_2 = Path(f'../ttmp/Chopin_Mazurkas_Modified/annotations_beat')\n",
    "    for hypdir in glob.glob(f'{rootdir}/{benchmark}/{system}'):\n",
    "        outpath = outdir + '/' + f'{benchmark}'\n",
    "        Path(outpath).mkdir(parents=True, exist_ok=True)\n",
    "        savefile = outpath + '/' + os.path.basename(hypdir) + '.pkl'\n",
    "        allErrs = eval_dir(hypdir, querylist, annot_root_1, annot_root_2, hop_sec, savefile = savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1f935",
   "metadata": {},
   "source": [
    "**Evaluate on Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37403ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_ROOT = f'experiments_train/{TRAIN_SET}'\n",
    "hop_sec = 512 * 1 / 22050\n",
    "outdir = f'evaluations_train/{TRAIN_SET}'\n",
    "Path(outdir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfed7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_benchmark(experiments_root, hop_sec, outdir, system, benchmark):\n",
    "    eval_all_dirs(EXPERIMENTS_ROOT, query_list, hop_sec, outdir, system, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMS = ['DTW1', 'DTW2', 'SubseqDTW']\n",
    "BENCHMARKS = ['Matching', 'Subseq10', 'Subseq30', 'PartialOverlap', 'Pre_5', 'Pre_10', 'Pre_15', 'Pre_20', 'Post_5', 'Post_10', 'Post_15', 'Post_20', 'Pre_Post_5', 'Pre_Post_10', 'Pre_Post_15', 'Pre_Post_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = [system.lower() for system in SYSTEMS]\n",
    "benchmarks = [benchmark.lower() if (benchmark != 'PartialOverlap') else 'partialOverlap' for benchmark in BENCHMARKS ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_benchmarks(experiments_root, hop_sec, outdir, systems, benchmarks):\n",
    "    for benchmark in benchmarks:\n",
    "        for system in systems:\n",
    "            eval_benchmark(experiments_root, hop_sec, outdir, system, benchmark)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d9d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_all_benchmarks(EXPERIMENTS_ROOT, hop_sec, outdir, systems, benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d80ae3",
   "metadata": {},
   "source": [
    "### Plot error vs tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates(errFile, maxTol):\n",
    "    \n",
    "    # read from file\n",
    "    with open(errFile, 'rb') as f:\n",
    "        allErrs = pickle.load(f)\n",
    "    \n",
    "    # collect all errors\n",
    "    errsFlat = []\n",
    "    for query in allErrs:\n",
    "        errs = np.array(allErrs[query])\n",
    "        errsFlat.append(errs)\n",
    "    errsFlat = np.concatenate(errsFlat)\n",
    "    \n",
    "    # calculate error rates\n",
    "    errRates = np.zeros(maxTol+1)\n",
    "    for i in range(maxTol+1):\n",
    "        errRates[i] = np.mean(np.abs(errsFlat) > i/1000)\n",
    "    \n",
    "    return errRates, errsFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates_batch(indir, basenames, maxTol):\n",
    "    errRates = np.zeros((len(basenames), maxTol+1))\n",
    "    allErrVals = []\n",
    "    print('Computing error rates ', end='')\n",
    "    for i, basename in enumerate(basenames):\n",
    "        errFile = indir + '/' + basename + '.pkl'\n",
    "        errRates[i,:], errors = calc_error_rates(errFile, maxTol)\n",
    "        allErrVals.append(errors)\n",
    "        print('.', end='')\n",
    "    print(' done')\n",
    "    return errRates, allErrVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_roc(errRates, basenames):\n",
    "    numSystems = errRates.shape[0]\n",
    "    maxTol = errRates.shape[1] - 1\n",
    "    for i in range(numSystems):\n",
    "        plt.plot(np.arange(maxTol+1), errRates[i,:] * 100.0)\n",
    "    plt.legend(basenames, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1bf0e",
   "metadata": {},
   "source": [
    "**Evaluate on Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04332674",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ROOT_DIR = f'evaluations_train/{TRAIN_SET}'\n",
    "toPlot = []\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    for system in systems:\n",
    "        toPlot.append(f'{benchmark}/{system}')\n",
    "maxTol = 1000 # in msec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errs(eval_root_dir, toPlot, maxTol):\n",
    "    errRates, errVals = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot, maxTol)\n",
    "    return errRates, errVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b760871",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates, errVals = get_errs(EVAL_ROOT_DIR, toPlot, maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_multiple_roc(errRates, toPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73e58e",
   "metadata": {},
   "source": [
    "### Make Plots (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(time, errRates):\n",
    "    data = {}\n",
    "    \n",
    "    data['Benchmark'] = []\n",
    "    for benchmark in BENCHMARKS:\n",
    "        data['Benchmark'] += [benchmark] * 3\n",
    "    \n",
    "    data['System'] = [system for system in SYSTEMS] * 16\n",
    "\n",
    "    data['Error'] = []\n",
    "    for i in range(0, 48, 3):\n",
    "        data['Error'].append(errRates[i][time]*100) # dtw1\n",
    "        data['Error'].append(errRates[i+1][time]*100) # dtw2\n",
    "        data['Error'].append(errRates[i+2][time]*100) # subseqdtw\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    benchmark_categories = CategoricalDtype(categories=BENCHMARKS, ordered=True)\n",
    "    df.Benchmark = df.Benchmark.astype(benchmark_categories)\n",
    "    system_categories = CategoricalDtype(categories=SYSTEMS, ordered=True)\n",
    "    df.System = df.System.astype(system_categories)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524aa0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms200_df = make_df(200, errRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0df3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#fde725' , '#35b779', '#31688e']\n",
    "ms100_df = make_df(100, errRates)\n",
    "ms500_df = make_df(500, errRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(ms200_df, aes(x=\"System\", y=\"Error\", fill=\"System\")) +\n",
    "    geom_bar(width = 0.7, position=position_dodge2(preserve='single', width=0.2), stat='identity') +\n",
    "    scale_y_continuous(expand = [0, 0], limits = [0, 100]) +\n",
    "    scale_fill_manual(values=colors) +\n",
    "    geom_crossbar(ms100_df, aes(ymin=\"Error\", ymax=\"Error\")) +\n",
    "    geom_crossbar(ms500_df, aes(ymin=\"Error\", ymax=\"Error\")) +\n",
    "    facet_grid('. ~ Benchmark') +\n",
    "    theme_bw() + \n",
    "    labs(y = \"Error Rate\") +\n",
    "    theme(dpi=300, legend_position=(0.5, -0.03), legend_direction=\"horizontal\", legend_title_align='bottom', \n",
    "            legend_background = element_blank(),\n",
    "            legend_title = element_text(size=10),\n",
    "            strip_background = element_blank(),\n",
    "            strip_text_x = element_text(angle = 50, size=7, position=(0.3, -0.07)),\n",
    "            axis_text_x = element_blank(),\n",
    "            axis_ticks_major_x = element_blank(),\n",
    "            axis_text_y = element_text(size = 10, colour='black'), \n",
    "            axis_title_x = element_blank(),\n",
    "            axis_title_y = element_text(size = 10, margin={'r': 6.0})) +\n",
    "    guides(fill=guide_legend(title=\"System:\", title_position='left', label_position=\"right\", override_aes = {'size': 0})))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
